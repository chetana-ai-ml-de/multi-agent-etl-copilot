{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":13795690,"sourceType":"datasetVersion","datasetId":8783295}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Multi-Agent ETL Copilot for Tabular Data (Kaggle Agents Capstone)\n\nThis notebook implements a **multi-agent ETL Copilot** using the Google Agent Development Kit (ADK).\n\nThe system:\n\n- Accepts a **source tabular dataset** (example: nurse staffing data).\n- Extracts and summarizes the **source schema**.\n- Maps the source schema to a **target analytics schema**.\n- Generates **pandas transformation code**.\n- Validates the code with a **ValidationAgent**.\n- Generates human-readable **Markdown documentation** with a **DocumentationAgent**.\n\nAlthough the example dataset is nurse staffing, the agents and prompts are designed to be **generic** and reusable for arbitrary tabular datasets (e.g., finance, operations, customer data).\n","metadata":{}},{"cell_type":"markdown","source":"## ðŸŸ¦ Step 1 â€” Load Required Libraries\nIn this step, I load the essential Python libraries used for data loading and exploration.\n\n- **pandas** for reading CSV files  \n- **numpy** for basic numerical operations  \n- additional libraries will be introduced when needed\n\nThis prepares the notebook environment for analyzing the nurse staffing dataset.\n","metadata":{}},{"cell_type":"markdown","source":"## ðŸŸ¦ Step 2 â€” Load the Nurse Staffing Dataset\nI load the nurse staffing CSV file provided in the Kaggle dataset.\n\nKaggle datasets are usually stored in:\n/kaggle/input","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-01T15:15:06.292020Z","iopub.execute_input":"2025-12-01T15:15:06.292323Z","iopub.status.idle":"2025-12-01T15:15:06.302328Z","shell.execute_reply.started":"2025-12-01T15:15:06.292289Z","shell.execute_reply":"2025-12-01T15:15:06.301577Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/nurse-staff-csv/PBJ_Daily_Nurse_Staffing_Q2_2024.csv\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"Once loaded, I preview the first few rows to understand its structure and content.\n## ðŸŸ¦ Step 3 â€” Preview the Dataset\nBefore building any agents, I preview the dataset using:\n\n- `df.head()` for sample rows  \n- `df.shape` for number of rows and columns  \n- `df.describe()` for basic numeric summaries  \n\nThis helps me confirm:\n- the dataset loaded correctly  \n- the column naming patterns  \n- numeric vs categorical fields  \n","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\ndf = pd.read_csv('/kaggle/input/nurse-staff-csv/PBJ_Daily_Nurse_Staffing_Q2_2024.csv', encoding='latin1')\ndf.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T15:15:10.124740Z","iopub.execute_input":"2025-12-01T15:15:10.125721Z","iopub.status.idle":"2025-12-01T15:15:15.086338Z","shell.execute_reply.started":"2025-12-01T15:15:10.125691Z","shell.execute_reply":"2025-12-01T15:15:15.085421Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_47/820383754.py:3: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv('/kaggle/input/nurse-staff-csv/PBJ_Daily_Nurse_Staffing_Q2_2024.csv', encoding='latin1')\n","output_type":"stream"},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"  PROVNUM                  PROVNAME          CITY STATE COUNTY_NAME  \\\n0   15009  BURNS NURSING HOME, INC.  RUSSELLVILLE    AL    Franklin   \n1   15009  BURNS NURSING HOME, INC.  RUSSELLVILLE    AL    Franklin   \n2   15009  BURNS NURSING HOME, INC.  RUSSELLVILLE    AL    Franklin   \n3   15009  BURNS NURSING HOME, INC.  RUSSELLVILLE    AL    Franklin   \n4   15009  BURNS NURSING HOME, INC.  RUSSELLVILLE    AL    Franklin   \n\n   COUNTY_FIPS  CY_Qtr  WorkDate  MDScensus  Hrs_RNDON  ...  Hrs_LPN_ctr  \\\n0           59  2024Q2  20240401         51      10.77  ...          0.0   \n1           59  2024Q2  20240402         52       8.43  ...          0.0   \n2           59  2024Q2  20240403         53      11.13  ...          0.0   \n3           59  2024Q2  20240404         52      12.27  ...          0.0   \n4           59  2024Q2  20240405         52       4.95  ...          0.0   \n\n   Hrs_CNA  Hrs_CNA_emp  Hrs_CNA_ctr  Hrs_NAtrn  Hrs_NAtrn_emp  Hrs_NAtrn_ctr  \\\n0   160.08       160.08          0.0        0.0            0.0            0.0   \n1   135.95       135.95          0.0        0.0            0.0            0.0   \n2   150.31       150.31          0.0        0.0            0.0            0.0   \n3   133.01       133.01          0.0        0.0            0.0            0.0   \n4   137.92       137.92          0.0        0.0            0.0            0.0   \n\n   Hrs_MedAide  Hrs_MedAide_emp  Hrs_MedAide_ctr  \n0          0.0              0.0              0.0  \n1          0.0              0.0              0.0  \n2          0.0              0.0              0.0  \n3          0.0              0.0              0.0  \n4          0.0              0.0              0.0  \n\n[5 rows x 33 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>PROVNUM</th>\n      <th>PROVNAME</th>\n      <th>CITY</th>\n      <th>STATE</th>\n      <th>COUNTY_NAME</th>\n      <th>COUNTY_FIPS</th>\n      <th>CY_Qtr</th>\n      <th>WorkDate</th>\n      <th>MDScensus</th>\n      <th>Hrs_RNDON</th>\n      <th>...</th>\n      <th>Hrs_LPN_ctr</th>\n      <th>Hrs_CNA</th>\n      <th>Hrs_CNA_emp</th>\n      <th>Hrs_CNA_ctr</th>\n      <th>Hrs_NAtrn</th>\n      <th>Hrs_NAtrn_emp</th>\n      <th>Hrs_NAtrn_ctr</th>\n      <th>Hrs_MedAide</th>\n      <th>Hrs_MedAide_emp</th>\n      <th>Hrs_MedAide_ctr</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>15009</td>\n      <td>BURNS NURSING HOME, INC.</td>\n      <td>RUSSELLVILLE</td>\n      <td>AL</td>\n      <td>Franklin</td>\n      <td>59</td>\n      <td>2024Q2</td>\n      <td>20240401</td>\n      <td>51</td>\n      <td>10.77</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>160.08</td>\n      <td>160.08</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>15009</td>\n      <td>BURNS NURSING HOME, INC.</td>\n      <td>RUSSELLVILLE</td>\n      <td>AL</td>\n      <td>Franklin</td>\n      <td>59</td>\n      <td>2024Q2</td>\n      <td>20240402</td>\n      <td>52</td>\n      <td>8.43</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>135.95</td>\n      <td>135.95</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>15009</td>\n      <td>BURNS NURSING HOME, INC.</td>\n      <td>RUSSELLVILLE</td>\n      <td>AL</td>\n      <td>Franklin</td>\n      <td>59</td>\n      <td>2024Q2</td>\n      <td>20240403</td>\n      <td>53</td>\n      <td>11.13</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>150.31</td>\n      <td>150.31</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>15009</td>\n      <td>BURNS NURSING HOME, INC.</td>\n      <td>RUSSELLVILLE</td>\n      <td>AL</td>\n      <td>Franklin</td>\n      <td>59</td>\n      <td>2024Q2</td>\n      <td>20240404</td>\n      <td>52</td>\n      <td>12.27</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>133.01</td>\n      <td>133.01</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>15009</td>\n      <td>BURNS NURSING HOME, INC.</td>\n      <td>RUSSELLVILLE</td>\n      <td>AL</td>\n      <td>Franklin</td>\n      <td>59</td>\n      <td>2024Q2</td>\n      <td>20240405</td>\n      <td>52</td>\n      <td>4.95</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>137.92</td>\n      <td>137.92</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 33 columns</p>\n</div>"},"metadata":{}}],"execution_count":4},{"cell_type":"markdown","source":"## ðŸŸ¦ Step 4 â€” Extract the Source Schema\nTo build an ETL Copilot, the first agent (SchemaAgent) must understand the dataset structure.\n\nIn this step, I extract:\n- column names  \n- data types  \n- non-null counts  \n\nI use `df.info()` for a quick view, and then a small helper tool to convert the schema into a structured Python list.\n\nThis is the first building block for the **SchemaAgent**.\n \n","metadata":{}},{"cell_type":"code","source":"df.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T15:15:15.087777Z","iopub.execute_input":"2025-12-01T15:15:15.088064Z","iopub.status.idle":"2025-12-01T15:15:15.489346Z","shell.execute_reply.started":"2025-12-01T15:15:15.088041Z","shell.execute_reply":"2025-12-01T15:15:15.488640Z"}},"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 1325324 entries, 0 to 1325323\nData columns (total 33 columns):\n #   Column            Non-Null Count    Dtype  \n---  ------            --------------    -----  \n 0   PROVNUM           1325324 non-null  object \n 1   PROVNAME          1325324 non-null  object \n 2   CITY              1325324 non-null  object \n 3   STATE             1325324 non-null  object \n 4   COUNTY_NAME       1325324 non-null  object \n 5   COUNTY_FIPS       1325324 non-null  int64  \n 6   CY_Qtr            1325324 non-null  object \n 7   WorkDate          1325324 non-null  int64  \n 8   MDScensus         1325324 non-null  int64  \n 9   Hrs_RNDON         1325324 non-null  float64\n 10  Hrs_RNDON_emp     1325324 non-null  float64\n 11  Hrs_RNDON_ctr     1325324 non-null  float64\n 12  Hrs_RNadmin       1325324 non-null  float64\n 13  Hrs_RNadmin_emp   1325324 non-null  float64\n 14  Hrs_RNadmin_ctr   1325324 non-null  float64\n 15  Hrs_RN            1325324 non-null  float64\n 16  Hrs_RN_emp        1325324 non-null  float64\n 17  Hrs_RN_ctr        1325324 non-null  float64\n 18  Hrs_LPNadmin      1325324 non-null  float64\n 19  Hrs_LPNadmin_emp  1325324 non-null  float64\n 20  Hrs_LPNadmin_ctr  1325324 non-null  float64\n 21  Hrs_LPN           1325324 non-null  float64\n 22  Hrs_LPN_emp       1325324 non-null  float64\n 23  Hrs_LPN_ctr       1325324 non-null  float64\n 24  Hrs_CNA           1325324 non-null  float64\n 25  Hrs_CNA_emp       1325324 non-null  float64\n 26  Hrs_CNA_ctr       1325324 non-null  float64\n 27  Hrs_NAtrn         1325324 non-null  float64\n 28  Hrs_NAtrn_emp     1325324 non-null  float64\n 29  Hrs_NAtrn_ctr     1325324 non-null  float64\n 30  Hrs_MedAide       1325324 non-null  float64\n 31  Hrs_MedAide_emp   1325324 non-null  float64\n 32  Hrs_MedAide_ctr   1325324 non-null  float64\ndtypes: float64(24), int64(3), object(6)\nmemory usage: 333.7+ MB\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# Helper: custom \"schema extraction tool\" for agents\n\ndef extract_schema(df: pd.DataFrame):\n    \"\"\"\n    Convert df.dtypes into a structured list of dicts.\n    This acts as a simple custom tool for the future SchemaAgent.\n    \"\"\"\n    schema = []\n    for col, dtype in df.dtypes.items():\n        schema.append({\n            \"column\": col,\n            \"dtype\": str(dtype)\n        })\n    return schema\n\nsource_schema = extract_schema(df)\nsource_schema[:10]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T15:15:22.655241Z","iopub.execute_input":"2025-12-01T15:15:22.655521Z","iopub.status.idle":"2025-12-01T15:15:22.662855Z","shell.execute_reply.started":"2025-12-01T15:15:22.655502Z","shell.execute_reply":"2025-12-01T15:15:22.662183Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"[{'column': 'PROVNUM', 'dtype': 'object'},\n {'column': 'PROVNAME', 'dtype': 'object'},\n {'column': 'CITY', 'dtype': 'object'},\n {'column': 'STATE', 'dtype': 'object'},\n {'column': 'COUNTY_NAME', 'dtype': 'object'},\n {'column': 'COUNTY_FIPS', 'dtype': 'int64'},\n {'column': 'CY_Qtr', 'dtype': 'object'},\n {'column': 'WorkDate', 'dtype': 'int64'},\n {'column': 'MDScensus', 'dtype': 'int64'},\n {'column': 'Hrs_RNDON', 'dtype': 'float64'}]"},"metadata":{}}],"execution_count":7},{"cell_type":"markdown","source":"## ðŸŸ¦ Step 5 â€” Convert Schema Into Prompt-Friendly Format\n\nLarge raw outputs (like `df.info()`) are not ideal for AI models.\n\nSo I perform **context engineering** by converting the schema into a compact bullet-list format:\n\n- `- column_name: dtype`\n\nThis makes it easier for an LLM-based agent to reason about mappings.\n\nThis is how I prepare the **context** for the Mapping Agent.\n","metadata":{}},{"cell_type":"code","source":"# Step 5 - Build a bullet-list version of the schema for LLM context\n\ndef schema_to_bullets(schema):\n    \"\"\"\n    Turn the schema list into a bullet-list string:\n    - COLUMN: dtype\n    \"\"\"\n    lines = [f\"- {col['column']}: {col['dtype']}\" for col in schema]\n    return \"\\n\".join(lines)\n\nsource_schema_text = schema_to_bullets(source_schema)\nprint(source_schema_text)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T15:15:27.326524Z","iopub.execute_input":"2025-12-01T15:15:27.326841Z","iopub.status.idle":"2025-12-01T15:15:27.332545Z","shell.execute_reply.started":"2025-12-01T15:15:27.326817Z","shell.execute_reply":"2025-12-01T15:15:27.331643Z"}},"outputs":[{"name":"stdout","text":"- PROVNUM: object\n- PROVNAME: object\n- CITY: object\n- STATE: object\n- COUNTY_NAME: object\n- COUNTY_FIPS: int64\n- CY_Qtr: object\n- WorkDate: int64\n- MDScensus: int64\n- Hrs_RNDON: float64\n- Hrs_RNDON_emp: float64\n- Hrs_RNDON_ctr: float64\n- Hrs_RNadmin: float64\n- Hrs_RNadmin_emp: float64\n- Hrs_RNadmin_ctr: float64\n- Hrs_RN: float64\n- Hrs_RN_emp: float64\n- Hrs_RN_ctr: float64\n- Hrs_LPNadmin: float64\n- Hrs_LPNadmin_emp: float64\n- Hrs_LPNadmin_ctr: float64\n- Hrs_LPN: float64\n- Hrs_LPN_emp: float64\n- Hrs_LPN_ctr: float64\n- Hrs_CNA: float64\n- Hrs_CNA_emp: float64\n- Hrs_CNA_ctr: float64\n- Hrs_NAtrn: float64\n- Hrs_NAtrn_emp: float64\n- Hrs_NAtrn_ctr: float64\n- Hrs_MedAide: float64\n- Hrs_MedAide_emp: float64\n- Hrs_MedAide_ctr: float64\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"## ðŸŸ¦ Step 6 â€” Define the Target Schema (Final Structure)\n\nTo drive ETL mapping, I define a **target schema**:  \nthe final cleaned structure I want the nurse staffing data to be transformed into.\n\n### ðŸ“Œ Target Schema\n\n```text\nfacility_id\nfacility_name\nstate\ncounty_name\nquarter\nwork_date\nresident_census\nrn_hours_total\nlpn_hours_total\ncna_hours_total\ntotal_nursing_hours\n","metadata":{}},{"cell_type":"markdown","source":"Field Definitions\n\nfacility_id â†’ unique facility identifier\n\nfacility_name â†’ name of the facility\n\nstate â†’ U.S. state\n\ncounty_name â†’ county name\n\nquarter â†’ quarter-year period (from CY_Qtr)\n\nwork_date â†’ report date (from WorkDate)\n\nresident_census â†’ number of residents (MDScensus)\n\nrn_hours_total â†’ combined RN-related hours\n\nlpn_hours_total â†’ combined LPN-related hours\n\ncna_hours_total â†’ combined CNA / aide hours\n\ntotal_nursing_hours â†’ RN + LPN + CNA totals\n\nThis target schema is the goal for the Mapping Agent.","metadata":{}},{"cell_type":"code","source":"\n# Step 6 - Represent target schema in code (for use in prompts / agents)\n\ntarget_schema_fields = [\n    \"facility_id\",\n    \"facility_name\",\n    \"state\",\n    \"county_name\",\n    \"quarter\",\n    \"work_date\",\n    \"resident_census\",\n    \"rn_hours_total\",\n    \"lpn_hours_total\",\n    \"cna_hours_total\",\n    \"total_nursing_hours\",\n]\n\ntarget_schema_text = \"\\n\".join(target_schema_fields)\nprint(target_schema_text)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T15:15:35.859929Z","iopub.execute_input":"2025-12-01T15:15:35.860658Z","iopub.status.idle":"2025-12-01T15:15:35.866254Z","shell.execute_reply.started":"2025-12-01T15:15:35.860626Z","shell.execute_reply":"2025-12-01T15:15:35.865092Z"}},"outputs":[{"name":"stdout","text":"facility_id\nfacility_name\nstate\ncounty_name\nquarter\nwork_date\nresident_census\nrn_hours_total\nlpn_hours_total\ncna_hours_total\ntotal_nursing_hours\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"## ðŸŸ¦ Step 7 â€” Build the Mapping Prompt (Source â†’ Target)\n\nNow I construct a **Mapping Prompt** that will be sent to the AI model (Gemini).\n\nThis prompt gives the model:\n\n1. The **source schema** (bullet-list form)\n2. The **target schema** (final structure)\n3. Clear instructions to:\n   - map each target field to the best source column\n   - indicate when multiple columns should be combined (e.g., RN + LPN + CNA hours)\n   - flag any missing matches\n   - explain its reasoning\n\nThis prompt powers the **MappingAgent** in the ETL Copilot.\n","metadata":{}},{"cell_type":"code","source":"# Step 7 - Build the mapping prompt text\n\nmapping_prompt = f\"\"\"\nYou are a Data Mapping Agent.\n\nHere is the source schema extracted from the dataset:\n{source_schema_text}\n\nHere is the target schema I want to map to:\n{target_schema_text}\n\nYour task:\n- Identify the best matching source column for each target field.\n- Mention if multiple columns must be combined (e.g., RN + LPN + CNA hours).\n- Mention if no matching source column exists.\n- Explain your reasoning in 1â€“2 lines per target field.\n- Do not hallucinate or invent new columns.\n- Keep response clean, readable, and ideally structured as bullet points.\n\"\"\"\n\nprint(\"=== MAPPING PROMPT PREVIEW (first 800 chars) ===\")\nprint(mapping_prompt[:800])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T15:15:39.671739Z","iopub.execute_input":"2025-12-01T15:15:39.672152Z","iopub.status.idle":"2025-12-01T15:15:39.677521Z","shell.execute_reply.started":"2025-12-01T15:15:39.672128Z","shell.execute_reply":"2025-12-01T15:15:39.676675Z"}},"outputs":[{"name":"stdout","text":"=== MAPPING PROMPT PREVIEW (first 800 chars) ===\n\nYou are a Data Mapping Agent.\n\nHere is the source schema extracted from the dataset:\n- PROVNUM: object\n- PROVNAME: object\n- CITY: object\n- STATE: object\n- COUNTY_NAME: object\n- COUNTY_FIPS: int64\n- CY_Qtr: object\n- WorkDate: int64\n- MDScensus: int64\n- Hrs_RNDON: float64\n- Hrs_RNDON_emp: float64\n- Hrs_RNDON_ctr: float64\n- Hrs_RNadmin: float64\n- Hrs_RNadmin_emp: float64\n- Hrs_RNadmin_ctr: float64\n- Hrs_RN: float64\n- Hrs_RN_emp: float64\n- Hrs_RN_ctr: float64\n- Hrs_LPNadmin: float64\n- Hrs_LPNadmin_emp: float64\n- Hrs_LPNadmin_ctr: float64\n- Hrs_LPN: float64\n- Hrs_LPN_emp: float64\n- Hrs_LPN_ctr: float64\n- Hrs_CNA: float64\n- Hrs_CNA_emp: float64\n- Hrs_CNA_ctr: float64\n- Hrs_NAtrn: float64\n- Hrs_NAtrn_emp: float64\n- Hrs_NAtrn_ctr: float64\n- Hrs_MedAide: float64\n- Hrs_MedAide_emp: float64\n- Hrs_Me\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"## ðŸŸ¦ Step 8 â€” AI Mapping Agent (Using ADK + Gemini)\n\nThis step implements the **MappingAgent** using the Agent Development Kit (ADK).\n\nThe MappingAgent:\n- Takes the source schema (bullet-list)\n- Takes the target schema\n- Uses an **LLM-powered ADK agent** (Gemini model)\n- Returns a structured mapping from source â†’ target fields\n- Explains reasoning for each field\n","metadata":{}},{"cell_type":"code","source":"# Step 8 - Generic MappingAgent for any tabular dataset\n\nimport os\nfrom kaggle_secrets import UserSecretsClient\n\nfrom google.adk.agents import LlmAgent\nfrom google.adk.models.google_llm import Gemini\nfrom google.adk.runners import InMemoryRunner\n\n# 1ï¸âƒ£ Load API key from Kaggle secrets\ntry:\n    GOOGLE_API_KEY = UserSecretsClient().get_secret(\"GOOGLE_API_KEY\")\n    os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY\n    print(\"ðŸ”‘ GOOGLE_API_KEY loaded from Kaggle Secrets\")\nexcept Exception as e:\n    print(\"âŒ ERROR: Please set GOOGLE_API_KEY in your Kaggle Secrets\", e)\n\n# 2ï¸âƒ£ Define a GENERIC MappingAgent\nmapping_agent = LlmAgent(\n    name=\"mapping_agent\",\n    model=Gemini(model=\"gemini-2.5-flash-lite\"),\n    instruction=\"\"\"\nYou are a professional schema mapping assistant for tabular data.\n\nYou are ALWAYS given:\n1) A SOURCE SCHEMA: a list of column names and data types from some tabular dataset.\n2) A TARGET SCHEMA: a list of desired output fields.\n\nYour job:\n- For EACH target field, decide how it can be derived from the source schema.\n- If it maps directly to a single source column, state that clearly.\n- If it requires multiple source columns (e.g., sum of several numeric columns), list them and say what to do.\n- If there is no reasonable source column, clearly say \"NO MATCH\".\n\nRules:\n- Never invent or assume columns that are not in the source schema.\n- Be concise but explicit: 1â€“2 lines of reasoning per target field.\n- Keep the response structured and easy to read.\n\"\"\"\n)\n\nprint(\"âœ… Generic MappingAgent created via ADK\")\n\n# 3ï¸âƒ£ Create a runner\nmapping_runner = InMemoryRunner(agent=mapping_agent)\nprint(\"ðŸš€ InMemoryRunner created for MappingAgent\")\n\n# 4ï¸âƒ£ Build a generic mapping prompt\nmapping_prompt = f\"\"\"\nYou are mapping from an arbitrary SOURCE schema to a TARGET schema.\n\nIn this run, the source schema comes from a specific dataset (for example, nurse staffing),\nbut you must treat it generically as just \"tabular data with columns\".\n\nSOURCE SCHEMA:\n{source_schema_text}\n\nTARGET SCHEMA:\n{target_schema_text}\n\nPlease:\n- For each TARGET field, map it to one or more SOURCE columns when possible.\n- Indicate column combinations explicitly (e.g., sum of A + B + C).\n- Mark missing mappings as NO MATCH.\n- Provide a short reasoning for each mapping.\n\"\"\"\n\nprint(\"ðŸ§¾ Prepared generic mapping prompt. Calling MappingAgent...\")\n\n# 5ï¸âƒ£ Call the agent (returns a LIST of events)\nmapping_events = await mapping_runner.run_debug(mapping_prompt)\n\n# 6ï¸âƒ£ Extract the final text from events\nmapping_response_text = \"\"\n\nfor event in mapping_events:\n    if hasattr(event, \"is_final_response\") and event.is_final_response():\n        if getattr(event, \"content\", None) and getattr(event.content, \"parts\", None):\n            mapping_response_text = \"\".join(\n                getattr(part, \"text\", \"\") for part in event.content.parts\n            )\n            break\n\nprint(\"=== ðŸ§  Generic Schema Mapping Output (MappingAgent) ===\")\nprint(mapping_response_text)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T15:15:44.613917Z","iopub.execute_input":"2025-12-01T15:15:44.614224Z","iopub.status.idle":"2025-12-01T15:15:46.753072Z","shell.execute_reply.started":"2025-12-01T15:15:44.614206Z","shell.execute_reply":"2025-12-01T15:15:46.752028Z"}},"outputs":[{"name":"stdout","text":"ðŸ”‘ GOOGLE_API_KEY loaded from Kaggle Secrets\nâœ… Generic MappingAgent created via ADK\nðŸš€ InMemoryRunner created for MappingAgent\nðŸ§¾ Prepared generic mapping prompt. Calling MappingAgent...\n\n ### Created new session: debug_session_id\n\nUser > \nYou are mapping from an arbitrary SOURCE schema to a TARGET schema.\n\nIn this run, the source schema comes from a specific dataset (for example, nurse staffing),\nbut you must treat it generically as just \"tabular data with columns\".\n\nSOURCE SCHEMA:\n- PROVNUM: object\n- PROVNAME: object\n- CITY: object\n- STATE: object\n- COUNTY_NAME: object\n- COUNTY_FIPS: int64\n- CY_Qtr: object\n- WorkDate: int64\n- MDScensus: int64\n- Hrs_RNDON: float64\n- Hrs_RNDON_emp: float64\n- Hrs_RNDON_ctr: float64\n- Hrs_RNadmin: float64\n- Hrs_RNadmin_emp: float64\n- Hrs_RNadmin_ctr: float64\n- Hrs_RN: float64\n- Hrs_RN_emp: float64\n- Hrs_RN_ctr: float64\n- Hrs_LPNadmin: float64\n- Hrs_LPNadmin_emp: float64\n- Hrs_LPNadmin_ctr: float64\n- Hrs_LPN: float64\n- Hrs_LPN_emp: float64\n- Hrs_LPN_ctr: float64\n- Hrs_CNA: float64\n- Hrs_CNA_emp: float64\n- Hrs_CNA_ctr: float64\n- Hrs_NAtrn: float64\n- Hrs_NAtrn_emp: float64\n- Hrs_NAtrn_ctr: float64\n- Hrs_MedAide: float64\n- Hrs_MedAide_emp: float64\n- Hrs_MedAide_ctr: float64\n\nTARGET SCHEMA:\nfacility_id\nfacility_name\nstate\ncounty_name\nquarter\nwork_date\nresident_census\nrn_hours_total\nlpn_hours_total\ncna_hours_total\ntotal_nursing_hours\n\nPlease:\n- For each TARGET field, map it to one or more SOURCE columns when possible.\n- Indicate column combinations explicitly (e.g., sum of A + B + C).\n- Mark missing mappings as NO MATCH.\n- Provide a short reasoning for each mapping.\n\nmapping_agent > facility_id: PROVNUM - Direct mapping from the provider number.\nfacility_name: PROVNAME - Direct mapping from the provider name.\nstate: STATE - Direct mapping from the state column.\ncounty_name: COUNTY_NAME - Direct mapping from the county name.\nquarter: CY_Qtr - Direct mapping from the quarter column.\nwork_date: WorkDate - Direct mapping from the work date.\nresident_census: MDScensus - Direct mapping from the medical/surgical census.\nrn_hours_total: Hrs_RN - Direct mapping from the total RN hours.\nlpn_hours_total: Hrs_LPN - Direct mapping from the total LPN hours.\ncna_hours_total: Hrs_CNA - Direct mapping from the total CNA hours.\ntotal_nursing_hours: sum of Hrs_RN + Hrs_LPN + Hrs_CNA + Hrs_MedAide + Hrs_NAtrn - Summing up all direct nursing staff hours.\n=== ðŸ§  Generic Schema Mapping Output (MappingAgent) ===\nfacility_id: PROVNUM - Direct mapping from the provider number.\nfacility_name: PROVNAME - Direct mapping from the provider name.\nstate: STATE - Direct mapping from the state column.\ncounty_name: COUNTY_NAME - Direct mapping from the county name.\nquarter: CY_Qtr - Direct mapping from the quarter column.\nwork_date: WorkDate - Direct mapping from the work date.\nresident_census: MDScensus - Direct mapping from the medical/surgical census.\nrn_hours_total: Hrs_RN - Direct mapping from the total RN hours.\nlpn_hours_total: Hrs_LPN - Direct mapping from the total LPN hours.\ncna_hours_total: Hrs_CNA - Direct mapping from the total CNA hours.\ntotal_nursing_hours: sum of Hrs_RN + Hrs_LPN + Hrs_CNA + Hrs_MedAide + Hrs_NAtrn - Summing up all direct nursing staff hours.\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"## ðŸŸ¦ Step 9 â€” Review & Interpret MappingAgent Output\n\nIn this step, I examine the output generated by the ADK MappingAgent.\n\nI review:\n- Which source fields were mapped to each target field\n- How the agent decided RN/LPN/CNA hour combinations\n- Whether any target fields were missing or unmapped\n- The quality of the reasoning and correctness\n\nThis completes the second agent in the multi-agent sequence:\n**SchemaAgent â†’ MappingAgent**\n\nThe final mapping text is displayed below for clarity and verification.\n","metadata":{}},{"cell_type":"markdown","source":"## ðŸŸ¦ Step 10 â€” TransformAgent (AI-Generated Transformation Logic)\n\nThe MappingAgent tells me *which* columns map to the target schema.\n\nNext, the TransformAgent generates **Python transformation code** that:\n\n- computes rn_hours_total  \n- computes lpn_hours_total  \n- computes cna_hours_total  \n- computes total_nursing_hours  \n- creates the final cleaned dataframe\n\nThis demonstrates:\n\n- **Sequential multi-agent flow**\n- **LLM agent generating executable code**\n- **Custom tool integration** (code executor)\n- A core concept from the Capstone rubric\n","metadata":{}},{"cell_type":"code","source":"# Step 10 - Generic TransformAgent (generate pandas transformation code)\n\nfrom google.adk.agents import LlmAgent\nfrom google.adk.models.google_llm import Gemini\nfrom google.adk.runners import InMemoryRunner\n\n# 1ï¸âƒ£ Define a GENERIC TransformAgent\ntransform_agent = LlmAgent(\n    name=\"transform_agent\",\n    model=Gemini(model=\"gemini-2.5-flash-lite\"),\n    instruction=\"\"\"\nYou are a Senior Data Engineer writing pandas ETL code for arbitrary tabular data.\n\nYou are given:\n- A mapping description between SOURCE columns and TARGET fields.\n- The source data is already loaded into a pandas DataFrame named `df`.\n- The target schema is a list of final column names.\n\nYour job:\n1. Write Python (pandas) code that:\n   - Starts from the existing `df` with the SOURCE columns.\n   - Creates the TARGET columns using only the mapped source columns.\n   - Performs any aggregations or combinations described in the mapping (e.g., sums).\n   - Outputs a new DataFrame named `df_out` that has EXACTLY the target schema columns in order.\n\n2. Do NOT invent columns that were not mentioned in the mapping.\n3. Assume this code will be pasted into a cell where `df` already exists.\n4. Return ONLY valid Python code (no explanation, no comments, no markdown).\n\"\"\"\n)\n\nprint(\"âœ… Generic TransformAgent created via ADK\")\n\n# 2ï¸âƒ£ Runner for TransformAgent\ntransform_runner = InMemoryRunner(agent=transform_agent)\nprint(\"ðŸš€ InMemoryRunner created for TransformAgent\")\n\n# 3ï¸âƒ£ Build prompt using the generic mapping result\ntransform_prompt = f\"\"\"\nYou are given this mapping between SOURCE columns and TARGET fields:\n\n{mapping_response_text}\n\nThe target schema fields are:\n\n{target_schema_text}\n\nGenerate the Python (pandas) transformation code according to your instructions.\nRemember:\n- Input DataFrame name: df\n- Output DataFrame name: df_out\n\"\"\"\n\nprint(\"ðŸ§¾ Prepared generic transform prompt. Calling TransformAgent...\")\n\n# 4ï¸âƒ£ Call the agent\ntransform_events = await transform_runner.run_debug(transform_prompt)\n\n# 5ï¸âƒ£ Extract final code text\ntransform_code_text = \"\"\n\nfor event in transform_events:\n    if hasattr(event, \"is_final_response\") and event.is_final_response():\n        if getattr(event, \"content\", None) and getattr(event.content, \"parts\", None):\n            transform_code_text = \"\".join(\n                getattr(part, \"text\", \"\") for part in event.content.parts\n            )\n            break\n\nprint(\"=== ðŸ§® Generated Generic Transformation Code (TransformAgent) ===\")\nprint(transform_code_text)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T15:16:15.815025Z","iopub.execute_input":"2025-12-01T15:16:15.815343Z","iopub.status.idle":"2025-12-01T15:16:17.620487Z","shell.execute_reply.started":"2025-12-01T15:16:15.815322Z","shell.execute_reply":"2025-12-01T15:16:17.619627Z"}},"outputs":[{"name":"stdout","text":"âœ… Generic TransformAgent created via ADK\nðŸš€ InMemoryRunner created for TransformAgent\nðŸ§¾ Prepared generic transform prompt. Calling TransformAgent...\n\n ### Created new session: debug_session_id\n\nUser > \nYou are given this mapping between SOURCE columns and TARGET fields:\n\nfacility_id: PROVNUM - Direct mapping from the provider number.\nfacility_name: PROVNAME - Direct mapping from the provider name.\nstate: STATE - Direct mapping from the state column.\ncounty_name: COUNTY_NAME - Direct mapping from the county name.\nquarter: CY_Qtr - Direct mapping from the quarter column.\nwork_date: WorkDate - Direct mapping from the work date.\nresident_census: MDScensus - Direct mapping from the medical/surgical census.\nrn_hours_total: Hrs_RN - Direct mapping from the total RN hours.\nlpn_hours_total: Hrs_LPN - Direct mapping from the total LPN hours.\ncna_hours_total: Hrs_CNA - Direct mapping from the total CNA hours.\ntotal_nursing_hours: sum of Hrs_RN + Hrs_LPN + Hrs_CNA + Hrs_MedAide + Hrs_NAtrn - Summing up all direct nursing staff hours.\n\nThe target schema fields are:\n\nfacility_id\nfacility_name\nstate\ncounty_name\nquarter\nwork_date\nresident_census\nrn_hours_total\nlpn_hours_total\ncna_hours_total\ntotal_nursing_hours\n\nGenerate the Python (pandas) transformation code according to your instructions.\nRemember:\n- Input DataFrame name: df\n- Output DataFrame name: df_out\n\ntransform_agent > ```python\ndf_out = df.rename(columns={\n    'PROVNUM': 'facility_id',\n    'PROVNAME': 'facility_name',\n    'STATE': 'state',\n    'COUNTY_NAME': 'county_name',\n    'CY_Qtr': 'quarter',\n    'WorkDate': 'work_date',\n    'MDScensus': 'resident_census',\n    'Hrs_RN': 'rn_hours_total',\n    'Hrs_LPN': 'lpn_hours_total',\n    'Hrs_CNA': 'cna_hours_total'\n})\n\ndf_out['total_nursing_hours'] = df_out['rn_hours_total'].fillna(0) + \\\n                               df_out['lpn_hours_total'].fillna(0) + \\\n                               df_out['cna_hours_total'].fillna(0)\n\n# Add other potential nursing hours columns if they exist in df\nif 'Hrs_MedAide' in df.columns:\n    df_out['total_nursing_hours'] += df['Hrs_MedAide'].fillna(0)\nif 'Hrs_NAtrn' in df.columns:\n    df_out['total_nursing_hours'] += df['Hrs_NAtrn'].fillna(0)\n\n# Ensure all target columns exist, even if not directly mapped or calculated\nfor col in ['facility_id', 'facility_name', 'state', 'county_name', 'quarter', 'work_date', 'resident_census', 'rn_hours_total', 'lpn_hours_total', 'cna_hours_total', 'total_nursing_hours']:\n    if col not in df_out.columns:\n        df_out[col] = None\n\ndf_out = df_out[['facility_id', 'facility_name', 'state', 'county_name', 'quarter', 'work_date', 'resident_census', 'rn_hours_total', 'lpn_hours_total', 'cna_hours_total', 'total_nursing_hours']]\n```\n=== ðŸ§® Generated Generic Transformation Code (TransformAgent) ===\n```python\ndf_out = df.rename(columns={\n    'PROVNUM': 'facility_id',\n    'PROVNAME': 'facility_name',\n    'STATE': 'state',\n    'COUNTY_NAME': 'county_name',\n    'CY_Qtr': 'quarter',\n    'WorkDate': 'work_date',\n    'MDScensus': 'resident_census',\n    'Hrs_RN': 'rn_hours_total',\n    'Hrs_LPN': 'lpn_hours_total',\n    'Hrs_CNA': 'cna_hours_total'\n})\n\ndf_out['total_nursing_hours'] = df_out['rn_hours_total'].fillna(0) + \\\n                               df_out['lpn_hours_total'].fillna(0) + \\\n                               df_out['cna_hours_total'].fillna(0)\n\n# Add other potential nursing hours columns if they exist in df\nif 'Hrs_MedAide' in df.columns:\n    df_out['total_nursing_hours'] += df['Hrs_MedAide'].fillna(0)\nif 'Hrs_NAtrn' in df.columns:\n    df_out['total_nursing_hours'] += df['Hrs_NAtrn'].fillna(0)\n\n# Ensure all target columns exist, even if not directly mapped or calculated\nfor col in ['facility_id', 'facility_name', 'state', 'county_name', 'quarter', 'work_date', 'resident_census', 'rn_hours_total', 'lpn_hours_total', 'cna_hours_total', 'total_nursing_hours']:\n    if col not in df_out.columns:\n        df_out[col] = None\n\ndf_out = df_out[['facility_id', 'facility_name', 'state', 'county_name', 'quarter', 'work_date', 'resident_census', 'rn_hours_total', 'lpn_hours_total', 'cna_hours_total', 'total_nursing_hours']]\n```\n","output_type":"stream"}],"execution_count":13},{"cell_type":"markdown","source":"## ðŸŸ¦ Step 11 â€” ValidationAgent (Check Correctness & Quality)\n\nOnce transformation logic is suggested by the TransformAgent,\nthe ValidationAgent evaluates:\n\n- whether required target fields are present\n- whether data types match expectations\n- whether hour totals are computed correctly\n- if any fields are missing or incorrectly mapped\n\nThis completes the 3-agent chain:\n**SchemaAgent â†’ MappingAgent â†’ TransformAgent â†’ ValidationAgent**\n","metadata":{}},{"cell_type":"code","source":"# Step 11 - Generic ValidationAgent (validate transformation code)\n\nvalidation_agent = LlmAgent(\n    name=\"validation_agent\",\n    model=Gemini(model=\"gemini-2.5-flash-lite\"),\n    instruction=\"\"\"\nYou are a strict ETL Validation Agent for arbitrary tabular data.\n\nYou are given:\n- Python (pandas) transformation code that starts from a DataFrame `df`\n  and creates a new DataFrame `df_out`.\n- The target schema: a list of required output columns.\n\nYour job:\n1. Check that the code defines `df_out` and that `df_out` includes ALL required target columns.\n2. Check that the code ONLY uses source columns that could reasonably exist\n   given the mapping description (if provided).\n3. Identify:\n   - Missing target columns\n   - Suspicious or undefined column references\n   - Obvious logic problems (e.g., using a field before defining it)\n\n4. Return a structured validation report with:\n   - PASS/FAIL for the overall transformation\n   - A list of issues found (if any)\n   - Concrete suggestions to fix them.\n\nYou do NOT execute the code; you are doing a static reasoning review.\n\"\"\"\n)\n\nprint(\"âœ… Generic ValidationAgent created via ADK\")\n\nvalidation_runner = InMemoryRunner(agent=validation_agent)\nprint(\"ðŸš€ InMemoryRunner created for ValidationAgent\")\n\n# Build validation prompt\nvalidation_prompt = f\"\"\"\nHere is the transformation code:\n\n{transform_code_text}\n\nThe expected target schema (columns in df_out) is:\n\n{target_schema_text}\n\nValidate this code and produce a structured PASS/FAIL report as per your instructions.\n\"\"\"\n\nprint(\"ðŸ§¾ Prepared generic validation prompt. Calling ValidationAgent...\")\n\n# Call the agent\nvalidation_events = await validation_runner.run_debug(validation_prompt)\n\n# Extract final validation text\nvalidation_output_text = \"\"\n\nfor event in validation_events:\n    if hasattr(event, \"is_final_response\") and event.is_final_response():\n        if getattr(event, \"content\", None) and getattr(event.content, \"parts\", None):\n            validation_output_text = \"\".join(\n                getattr(part, \"text\", \"\") for part in event.content.parts\n            )\n            break\n\nprint(\"=== âœ… Generic Validation Report (ValidationAgent) ===\")\nprint(validation_output_text)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T15:16:37.212691Z","iopub.execute_input":"2025-12-01T15:16:37.213056Z","iopub.status.idle":"2025-12-01T15:16:41.208412Z","shell.execute_reply.started":"2025-12-01T15:16:37.213033Z","shell.execute_reply":"2025-12-01T15:16:41.207395Z"}},"outputs":[{"name":"stdout","text":"âœ… Generic ValidationAgent created via ADK\nðŸš€ InMemoryRunner created for ValidationAgent\nðŸ§¾ Prepared generic validation prompt. Calling ValidationAgent...\n\n ### Created new session: debug_session_id\n\nUser > \nHere is the transformation code:\n\n```python\ndf_out = df.rename(columns={\n    'PROVNUM': 'facility_id',\n    'PROVNAME': 'facility_name',\n    'STATE': 'state',\n    'COUNTY_NAME': 'county_name',\n    'CY_Qtr': 'quarter',\n    'WorkDate': 'work_date',\n    'MDScensus': 'resident_census',\n    'Hrs_RN': 'rn_hours_total',\n    'Hrs_LPN': 'lpn_hours_total',\n    'Hrs_CNA': 'cna_hours_total'\n})\n\ndf_out['total_nursing_hours'] = df_out['rn_hours_total'].fillna(0) + \\\n                               df_out['lpn_hours_total'].fillna(0) + \\\n                               df_out['cna_hours_total'].fillna(0)\n\n# Add other potential nursing hours columns if they exist in df\nif 'Hrs_MedAide' in df.columns:\n    df_out['total_nursing_hours'] += df['Hrs_MedAide'].fillna(0)\nif 'Hrs_NAtrn' in df.columns:\n    df_out['total_nursing_hours'] += df['Hrs_NAtrn'].fillna(0)\n\n# Ensure all target columns exist, even if not directly mapped or calculated\nfor col in ['facility_id', 'facility_name', 'state', 'county_name', 'quarter', 'work_date', 'resident_census', 'rn_hours_total', 'lpn_hours_total', 'cna_hours_total', 'total_nursing_hours']:\n    if col not in df_out.columns:\n        df_out[col] = None\n\ndf_out = df_out[['facility_id', 'facility_name', 'state', 'county_name', 'quarter', 'work_date', 'resident_census', 'rn_hours_total', 'lpn_hours_total', 'cna_hours_total', 'total_nursing_hours']]\n```\n\nThe expected target schema (columns in df_out) is:\n\nfacility_id\nfacility_name\nstate\ncounty_name\nquarter\nwork_date\nresident_census\nrn_hours_total\nlpn_hours_total\ncna_hours_total\ntotal_nursing_hours\n\nValidate this code and produce a structured PASS/FAIL report as per your instructions.\n\nvalidation_agent > ```json\n{\n  \"validation_result\": \"FAIL\",\n  \"issues\": [\n    {\n      \"issue_type\": \"Missing Target Columns\",\n      \"description\": \"The final `df_out` DataFrame is reordered to include columns not present in the target schema. Specifically, the line `df_out = df_out[['facility_id', 'facility_name', 'state', 'county_name', 'quarter', 'work_date', 'resident_census', 'rn_hours_total', 'lpn_hours_total', 'cna_hours_total', 'total_nursing_hours']]` explicitly selects only these columns. However, the target schema also expects `total_nursing_hours` which is calculated, but the explicit column selection at the end removes other columns that might have been intended to be present or were implicitly expected.\",\n      \"suggestions\": [\n        \"The final column selection `df_out = df_out[['facility_id', 'facility_name', 'state', 'county_name', 'quarter', 'work_date', 'resident_census', 'rn_hours_total', 'lpn_hours_total', 'cna_hours_total', 'total_nursing_hours']]` should be reviewed. If the intention was to ensure only these specific columns exist and in this order, and no other columns from the original `df` or intermediate calculations are expected, then this step is correct. However, the prompt implies that all target columns *must* be present, and the list provided for the final selection matches the target schema exactly. The issue arises if any of the target columns were *not* successfully mapped or calculated, which is handled by the `if col not in df_out.columns:` loop. The current code seems to have a slight redundancy/potential for confusion by first ensuring all target columns exist and then explicitly selecting them. If the intention is just to have the target columns, the loop `for col in [...]` is sufficient if the final selection is removed. If the final selection is *required* to enforce order and presence, then the loop might be redundant for columns present in the selection. The core issue is that the loop *adds* `None` for missing target columns, and then the final `df_out = df_out[...]` might overwrite or remove these if the column was already present but the loop was meant as a safeguard.  Assuming the target schema is the absolute definitive list, and the loop is to ensure *all* of them are there, then the final column selection might be problematic if it inadvertently excludes columns that the loop intended to add or preserve.\",\n        \"Consider removing the final column selection `df_out = df_out[['facility_id', 'facility_name', 'state', 'county_name', 'quarter', 'work_date', 'resident_census', 'rn_hours_total', 'lpn_hours_total', 'cna_hours_total', 'total_nursing_hours']]` if the order is not strictly enforced by the target schema, and rely on the preceding loop to ensure all target columns are present. If order *is* critical, then ensure the list in the final selection precisely matches the target schema and that all required columns are indeed generated before this step.\"\n      ]\n    },\n    {\n      \"issue_type\": \"Suspicious Column Reference\",\n      \"description\": \"The code references columns `Hrs_MedAide` and `Hrs_NAtrn` within `if` conditions to add to `total_nursing_hours`. These columns are not part of the initial `df.rename` mapping, nor are they explicitly mentioned in the target schema definition. While the code defensively checks for their existence before using them, their presence and intended inclusion in `total_nursing_hours` are not explicitly mapped to the target schema. If these columns are intended to be part of the output or the calculation of `total_nursing_hours` and are not mapped, they represent an implicit dependency that is not clearly defined.\",\n      \"suggestions\": [\n        \"Clarify if `Hrs_MedAide` and `Hrs_NAtrn` are intended to be source columns that should be mapped to new columns in `df_out` or if they are temporary calculation fields that should not persist or be referenced in the final schema. If they are intended to contribute to `total_nursing_hours` and should be part of the output, they need to be explicitly mapped or included in the final column selection.\",\n        \"If `Hrs_MedAide` and `Hrs_NAtrn` are only for the calculation of `total_nursing_hours` and do not need to be present as separate columns in the output, ensure that `total_nursing_hours` correctly aggregates them and that no other logic expects them as distinct output columns.\",\n        \"If these columns are supposed to be part of the source data but are missing from the initial rename, consider adding them to the `df.rename` dictionary if they are expected in the output with different names.\"\n      ]\n    }\n  ],\n  \"overall_status\": \"FAIL\"\n}\n```\n=== âœ… Generic Validation Report (ValidationAgent) ===\n```json\n{\n  \"validation_result\": \"FAIL\",\n  \"issues\": [\n    {\n      \"issue_type\": \"Missing Target Columns\",\n      \"description\": \"The final `df_out` DataFrame is reordered to include columns not present in the target schema. Specifically, the line `df_out = df_out[['facility_id', 'facility_name', 'state', 'county_name', 'quarter', 'work_date', 'resident_census', 'rn_hours_total', 'lpn_hours_total', 'cna_hours_total', 'total_nursing_hours']]` explicitly selects only these columns. However, the target schema also expects `total_nursing_hours` which is calculated, but the explicit column selection at the end removes other columns that might have been intended to be present or were implicitly expected.\",\n      \"suggestions\": [\n        \"The final column selection `df_out = df_out[['facility_id', 'facility_name', 'state', 'county_name', 'quarter', 'work_date', 'resident_census', 'rn_hours_total', 'lpn_hours_total', 'cna_hours_total', 'total_nursing_hours']]` should be reviewed. If the intention was to ensure only these specific columns exist and in this order, and no other columns from the original `df` or intermediate calculations are expected, then this step is correct. However, the prompt implies that all target columns *must* be present, and the list provided for the final selection matches the target schema exactly. The issue arises if any of the target columns were *not* successfully mapped or calculated, which is handled by the `if col not in df_out.columns:` loop. The current code seems to have a slight redundancy/potential for confusion by first ensuring all target columns exist and then explicitly selecting them. If the intention is just to have the target columns, the loop `for col in [...]` is sufficient if the final selection is removed. If the final selection is *required* to enforce order and presence, then the loop might be redundant for columns present in the selection. The core issue is that the loop *adds* `None` for missing target columns, and then the final `df_out = df_out[...]` might overwrite or remove these if the column was already present but the loop was meant as a safeguard.  Assuming the target schema is the absolute definitive list, and the loop is to ensure *all* of them are there, then the final column selection might be problematic if it inadvertently excludes columns that the loop intended to add or preserve.\",\n        \"Consider removing the final column selection `df_out = df_out[['facility_id', 'facility_name', 'state', 'county_name', 'quarter', 'work_date', 'resident_census', 'rn_hours_total', 'lpn_hours_total', 'cna_hours_total', 'total_nursing_hours']]` if the order is not strictly enforced by the target schema, and rely on the preceding loop to ensure all target columns are present. If order *is* critical, then ensure the list in the final selection precisely matches the target schema and that all required columns are indeed generated before this step.\"\n      ]\n    },\n    {\n      \"issue_type\": \"Suspicious Column Reference\",\n      \"description\": \"The code references columns `Hrs_MedAide` and `Hrs_NAtrn` within `if` conditions to add to `total_nursing_hours`. These columns are not part of the initial `df.rename` mapping, nor are they explicitly mentioned in the target schema definition. While the code defensively checks for their existence before using them, their presence and intended inclusion in `total_nursing_hours` are not explicitly mapped to the target schema. If these columns are intended to be part of the output or the calculation of `total_nursing_hours` and are not mapped, they represent an implicit dependency that is not clearly defined.\",\n      \"suggestions\": [\n        \"Clarify if `Hrs_MedAide` and `Hrs_NAtrn` are intended to be source columns that should be mapped to new columns in `df_out` or if they are temporary calculation fields that should not persist or be referenced in the final schema. If they are intended to contribute to `total_nursing_hours` and should be part of the output, they need to be explicitly mapped or included in the final column selection.\",\n        \"If `Hrs_MedAide` and `Hrs_NAtrn` are only for the calculation of `total_nursing_hours` and do not need to be present as separate columns in the output, ensure that `total_nursing_hours` correctly aggregates them and that no other logic expects them as distinct output columns.\",\n        \"If these columns are supposed to be part of the source data but are missing from the initial rename, consider adding them to the `df.rename` dictionary if they are expected in the output with different names.\"\n      ]\n    }\n  ],\n  \"overall_status\": \"FAIL\"\n}\n```\n","output_type":"stream"}],"execution_count":14},{"cell_type":"markdown","source":"### ðŸ§ª Using ValidationAgent Feedback\n\nThe initial TransformAgent code:\n- Included nurse-specific columns (`Hrs_MedAide`, `Hrs_NAtrn`) that were not in the target schema.\n- Used redundant logic (loop + explicit column selection).\n\nValidationAgent correctly flagged this with a FAIL and suggested improvements.\n\nBased on that feedback, I simplified the transformation:\n- Only mapped columns from `df` are used.\n- `total_nursing_hours` is computed as RN + LPN + CNA totals.\n- The final `df_out` DataFrame contains exactly the target schema columns.\n","metadata":{}},{"cell_type":"code","source":"### Final cleaned transformation code (after ValidationAgent feedback)\nimport pandas as pd\n\n# Start from the raw df and rename source columns to target-friendly names\ndf_out = df.rename(columns={\n    'PROVNUM': 'facility_id',\n    'PROVNAME': 'facility_name',\n    'STATE': 'state',\n    'COUNTY_NAME': 'county_name',\n    'CY_Qtr': 'quarter',\n    'WorkDate': 'work_date',\n    'MDScensus': 'resident_census',\n    'Hrs_RN': 'rn_hours_total',\n    'Hrs_LPN': 'lpn_hours_total',\n    'Hrs_CNA': 'cna_hours_total'\n}).copy()\n\n# Ensure hour fields exist and fill missing values with 0 before summing\nfor col in ['rn_hours_total', 'lpn_hours_total', 'cna_hours_total']:\n    if col not in df_out.columns:\n        df_out[col] = 0\n    df_out[col] = df_out[col].fillna(0)\n\n# Compute total nursing hours as the sum of RN, LPN and CNA hours\ndf_out['total_nursing_hours'] = (\n    df_out['rn_hours_total'] +\n    df_out['lpn_hours_total'] +\n    df_out['cna_hours_total']\n)\n\n# Final output: only the target schema columns, in the desired order\ndf_out = df_out[\n    [\n        'facility_id',\n        'facility_name',\n        'state',\n        'county_name',\n        'quarter',\n        'work_date',\n        'resident_census',\n        'rn_hours_total',\n        'lpn_hours_total',\n        'cna_hours_total',\n        'total_nursing_hours'\n    ]\n]\nprint(\"df_out shape:\", df_out.shape)\ndf_out.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T15:46:13.548070Z","iopub.execute_input":"2025-12-01T15:46:13.548583Z","iopub.status.idle":"2025-12-01T15:46:14.037514Z","shell.execute_reply.started":"2025-12-01T15:46:13.548556Z","shell.execute_reply":"2025-12-01T15:46:14.036774Z"}},"outputs":[{"name":"stdout","text":"df_out shape: (1325324, 11)\n","output_type":"stream"},{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"  facility_id             facility_name state county_name quarter  work_date  \\\n0       15009  BURNS NURSING HOME, INC.    AL    Franklin  2024Q2   20240401   \n1       15009  BURNS NURSING HOME, INC.    AL    Franklin  2024Q2   20240402   \n2       15009  BURNS NURSING HOME, INC.    AL    Franklin  2024Q2   20240403   \n3       15009  BURNS NURSING HOME, INC.    AL    Franklin  2024Q2   20240404   \n4       15009  BURNS NURSING HOME, INC.    AL    Franklin  2024Q2   20240405   \n\n   resident_census  rn_hours_total  lpn_hours_total  cna_hours_total  \\\n0               51           55.70            25.50           160.08   \n1               52           63.28            15.22           135.95   \n2               53           76.29             5.46           150.31   \n3               52           54.13            20.18           133.01   \n4               52           53.63            27.85           137.92   \n\n   total_nursing_hours  \n0               241.28  \n1               214.45  \n2               232.06  \n3               207.32  \n4               219.40  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>facility_id</th>\n      <th>facility_name</th>\n      <th>state</th>\n      <th>county_name</th>\n      <th>quarter</th>\n      <th>work_date</th>\n      <th>resident_census</th>\n      <th>rn_hours_total</th>\n      <th>lpn_hours_total</th>\n      <th>cna_hours_total</th>\n      <th>total_nursing_hours</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>15009</td>\n      <td>BURNS NURSING HOME, INC.</td>\n      <td>AL</td>\n      <td>Franklin</td>\n      <td>2024Q2</td>\n      <td>20240401</td>\n      <td>51</td>\n      <td>55.70</td>\n      <td>25.50</td>\n      <td>160.08</td>\n      <td>241.28</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>15009</td>\n      <td>BURNS NURSING HOME, INC.</td>\n      <td>AL</td>\n      <td>Franklin</td>\n      <td>2024Q2</td>\n      <td>20240402</td>\n      <td>52</td>\n      <td>63.28</td>\n      <td>15.22</td>\n      <td>135.95</td>\n      <td>214.45</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>15009</td>\n      <td>BURNS NURSING HOME, INC.</td>\n      <td>AL</td>\n      <td>Franklin</td>\n      <td>2024Q2</td>\n      <td>20240403</td>\n      <td>53</td>\n      <td>76.29</td>\n      <td>5.46</td>\n      <td>150.31</td>\n      <td>232.06</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>15009</td>\n      <td>BURNS NURSING HOME, INC.</td>\n      <td>AL</td>\n      <td>Franklin</td>\n      <td>2024Q2</td>\n      <td>20240404</td>\n      <td>52</td>\n      <td>54.13</td>\n      <td>20.18</td>\n      <td>133.01</td>\n      <td>207.32</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>15009</td>\n      <td>BURNS NURSING HOME, INC.</td>\n      <td>AL</td>\n      <td>Franklin</td>\n      <td>2024Q2</td>\n      <td>20240405</td>\n      <td>52</td>\n      <td>53.63</td>\n      <td>27.85</td>\n      <td>137.92</td>\n      <td>219.40</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":18},{"cell_type":"markdown","source":"## ðŸŸ¦ Step 12 â€” Multi-Agent Orchestrator (Mapping â†’ Transform â†’ Validation)\n\nIn this step, I connect the three agents into a single pipeline:\n\n1. **MappingAgent**  \n   - Maps the source schema to the target schema.\n\n2. **TransformAgent**  \n   - Generates pandas transformation code based on the mapping.\n\n3. **ValidationAgent**  \n   - Reviews the generated code and returns a PASS/FAIL-style report with issues and suggestions.\n\nThe orchestrator runs all three in sequence and returns a dictionary with:\n- `mapping` â€” human-readable mapping output  \n- `transform_code` â€” generated pandas code (first draft)  \n- `validation` â€” validation report for that code  \n","metadata":{}},{"cell_type":"code","source":"# Step 12 â€” Orchestrator for the 3-Agent ETL Pipeline\n\nimport asyncio\nimport nest_asyncio\nnest_asyncio.apply()\n\nasync def run_etl_orchestrator(source_schema_text: str, target_schema_text: str):\n    \"\"\"\n    Runs the 3-agent ETL pipeline:\n    1. MappingAgent  -> mapping_text\n    2. TransformAgent -> transform_code (first draft)\n    3. ValidationAgent -> validation_report\n    \"\"\"\n\n    print(\"\\nðŸš€ Starting ETL Orchestrator...\\n\")\n\n    # ======================\n    # 1ï¸âƒ£ MAPPING AGENT\n    # ======================\n    print(\"â–¶ï¸ Running MappingAgent...\")\n\n    mapping_prompt = f\"\"\"\nMap the following SOURCE schema to the TARGET schema.\n\nSOURCE SCHEMA:\n{source_schema_text}\n\nTARGET SCHEMA:\n{target_schema_text}\n\nRules:\n- For each TARGET field, map it to one or more SOURCE columns.\n- If combining columns, explicitly state which ones.\n- If no mapping exists, say NO MATCH.\n- Do not invent columns that do not exist in the source schema.\n\"\"\"\n\n    mapping_events = await mapping_runner.run_debug(mapping_prompt)\n\n    mapping_text = \"\"\n    for event in mapping_events:\n        if hasattr(event, \"is_final_response\") and event.is_final_response():\n            if getattr(event, \"content\", None) and event.content.parts:\n                mapping_text = \"\".join(getattr(p, \"text\", \"\") for p in event.content.parts)\n                break\n\n    print(\"âœ“ MappingAgent complete.\\n\")\n\n\n    # ======================\n    # 2ï¸âƒ£ TRANSFORM AGENT\n    # ======================\n    print(\"â–¶ï¸ Running TransformAgent...\")\n\n    transform_prompt = f\"\"\"\nYou are a Senior Data Engineer.\n\nHere is the mapping between SOURCE columns and TARGET fields:\n\n{mapping_text}\n\nThe target schema fields are:\n\n{target_schema_text}\n\nGenerate Python (pandas) code that:\n- Starts from a DataFrame named `df` with the SOURCE columns.\n- Produces a DataFrame named `df_out` with exactly the TARGET columns.\n- Uses only mapped source columns.\n- Returns only valid Python code (no comments, no markdown).\n\"\"\"\n\n    transform_events = await transform_runner.run_debug(transform_prompt)\n\n    transform_code = \"\"\n    for event in transform_events:\n        if hasattr(event, \"is_final_response\") and event.is_final_response():\n            if getattr(event, \"content\", None) and event.content.parts:\n                transform_code = \"\".join(getattr(p, \"text\", \"\") for p in event.content.parts)\n                break\n\n    print(\"âœ“ TransformAgent complete.\\n\")\n\n\n    # ======================\n    # 3ï¸âƒ£ VALIDATION AGENT\n    # ======================\n    print(\"â–¶ï¸ Running ValidationAgent...\")\n\n    validation_prompt = f\"\"\"\nHere is the generated pandas transformation code:\n\n{transform_code}\n\nThe expected target schema (columns in df_out) is:\n\n{target_schema_text}\n\nValidate this code statically and produce a structured PASS/FAIL report with:\n- Issues (e.g., missing columns, suspicious references)\n- Suggestions for how to fix them.\n\"\"\"\n\n    validation_events = await validation_runner.run_debug(validation_prompt)\n\n    validation_report = \"\"\n    for event in validation_events:\n        if hasattr(event, \"is_final_response\") and event.is_final_response():\n            if getattr(event, \"content\", None) and event.content.parts:\n                validation_report = \"\".join(\n                    getattr(p, \"text\", \"\") for p in event.content.parts\n                )\n                break\n\n    print(\"âœ“ ValidationAgent complete.\\n\")\n    print(\"ðŸŽ‰ ETL Orchestrator Finished!\")\n\n    # Return all outputs\n    return {\n        \"mapping\": mapping_text,\n        \"transform_code\": transform_code,\n        \"validation\": validation_report,\n    }\n\n\n# â­ Run the orchestrator once to capture outputs\nloop = asyncio.get_event_loop()\npipeline_output = loop.run_until_complete(\n    run_etl_orchestrator(source_schema_text, target_schema_text)\n)\n\npipeline_output\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T15:57:34.205764Z","iopub.execute_input":"2025-12-01T15:57:34.206067Z","iopub.status.idle":"2025-12-01T15:57:39.425760Z","shell.execute_reply.started":"2025-12-01T15:57:34.206048Z","shell.execute_reply":"2025-12-01T15:57:39.424839Z"}},"outputs":[{"name":"stdout","text":"\nðŸš€ Starting ETL Orchestrator...\n\nâ–¶ï¸ Running MappingAgent...\n\n ### Continue session: debug_session_id\n\nUser > \nMap the following SOURCE schema to the TARGET schema.\n\nSOURCE SCHEMA:\n- PROVNUM: object\n- PROVNAME: object\n- CITY: object\n- STATE: object\n- COUNTY_NAME: object\n- COUNTY_FIPS: int64\n- CY_Qtr: object\n- WorkDate: int64\n- MDScensus: int64\n- Hrs_RNDON: float64\n- Hrs_RNDON_emp: float64\n- Hrs_RNDON_ctr: float64\n- Hrs_RNadmin: float64\n- Hrs_RNadmin_emp: float64\n- Hrs_RNadmin_ctr: float64\n- Hrs_RN: float64\n- Hrs_RN_emp: float64\n- Hrs_RN_ctr: float64\n- Hrs_LPNadmin: float64\n- Hrs_LPNadmin_emp: float64\n- Hrs_LPNadmin_ctr: float64\n- Hrs_LPN: float64\n- Hrs_LPN_emp: float64\n- Hrs_LPN_ctr: float64\n- Hrs_CNA: float64\n- Hrs_CNA_emp: float64\n- Hrs_CNA_ctr: float64\n- Hrs_NAtrn: float64\n- Hrs_NAtrn_emp: float64\n- Hrs_NAtrn_ctr: float64\n- Hrs_MedAide: float64\n- Hrs_MedAide_emp: float64\n- Hrs_MedAide_ctr: float64\n\nTARGET SCHEMA:\nfacility_id\nfacility_name\nstate\ncounty_name\nquarter\nwork_date\nresident_census\nrn_hours_total\nlpn_hours_total\ncna_hours_total\ntotal_nursing_hours\n\nRules:\n- For each TARGET field, map it to one or more SOURCE columns.\n- If combining columns, explicitly state which ones.\n- If no mapping exists, say NO MATCH.\n- Do not invent columns that do not exist in the source schema.\n\nmapping_agent > facility_id: PROVNUM - Direct mapping from the provider number.\nfacility_name: PROVNAME - Direct mapping from the provider name.\nstate: STATE - Direct mapping from the state column.\ncounty_name: COUNTY_NAME - Direct mapping from the county name.\nquarter: CY_Qtr - Direct mapping from the quarter column.\nwork_date: WorkDate - Direct mapping from the work date.\nresident_census: MDScensus - Direct mapping from the medical/surgical census.\nrn_hours_total: Hrs_RN - Direct mapping from the total RN hours.\nlpn_hours_total: Hrs_LPN - Direct mapping from the total LPN hours.\ncna_hours_total: Hrs_CNA - Direct mapping from the total CNA hours.\ntotal_nursing_hours: sum of Hrs_RN + Hrs_LPN + Hrs_CNA + Hrs_MedAide + Hrs_NAtrn - Summing up all direct nursing staff hours.\nâœ“ MappingAgent complete.\n\nâ–¶ï¸ Running TransformAgent...\n\n ### Continue session: debug_session_id\n\nUser > \nYou are a Senior Data Engineer.\n\nHere is the mapping between SOURCE columns and TARGET fields:\n\nfacility_id: PROVNUM - Direct mapping from the provider number.\nfacility_name: PROVNAME - Direct mapping from the provider name.\nstate: STATE - Direct mapping from the state column.\ncounty_name: COUNTY_NAME - Direct mapping from the county name.\nquarter: CY_Qtr - Direct mapping from the quarter column.\nwork_date: WorkDate - Direct mapping from the work date.\nresident_census: MDScensus - Direct mapping from the medical/surgical census.\nrn_hours_total: Hrs_RN - Direct mapping from the total RN hours.\nlpn_hours_total: Hrs_LPN - Direct mapping from the total LPN hours.\ncna_hours_total: Hrs_CNA - Direct mapping from the total CNA hours.\ntotal_nursing_hours: sum of Hrs_RN + Hrs_LPN + Hrs_CNA + Hrs_MedAide + Hrs_NAtrn - Summing up all direct nursing staff hours.\n\nThe target schema fields are:\n\nfacility_id\nfacility_name\nstate\ncounty_name\nquarter\nwork_date\nresident_census\nrn_hours_total\nlpn_hours_total\ncna_hours_total\ntotal_nursing_hours\n\nGenerate Python (pandas) code that:\n- Starts from a DataFrame named `df` with the SOURCE columns.\n- Produces a DataFrame named `df_out` with exactly the TARGET columns.\n- Uses only mapped source columns.\n- Returns only valid Python code (no comments, no markdown).\n\ntransform_agent > ```python\ndf_out = df.rename(columns={\n    'PROVNUM': 'facility_id',\n    'PROVNAME': 'facility_name',\n    'STATE': 'state',\n    'COUNTY_NAME': 'county_name',\n    'CY_Qtr': 'quarter',\n    'WorkDate': 'work_date',\n    'MDScensus': 'resident_census',\n    'Hrs_RN': 'rn_hours_total',\n    'Hrs_LPN': 'lpn_hours_total',\n    'Hrs_CNA': 'cna_hours_total'\n})\n\n# Calculate total_nursing_hours by summing relevant columns, filling NaN with 0\nnursing_hour_cols = ['Hrs_RN', 'Hrs_LPN', 'Hrs_CNA']\nif 'Hrs_MedAide' in df.columns:\n    nursing_hour_cols.append('Hrs_MedAide')\nif 'Hrs_NAtrn' in df.columns:\n    nursing_hour_cols.append('Hrs_NAtrn')\n\ndf_out['total_nursing_hours'] = df[nursing_hour_cols].fillna(0).sum(axis=1)\n\n# Ensure all target columns are present and in the correct order\ntarget_columns = [\n    'facility_id',\n    'facility_name',\n    'state',\n    'county_name',\n    'quarter',\n    'work_date',\n    'resident_census',\n    'rn_hours_total',\n    'lpn_hours_total',\n    'cna_hours_total',\n    'total_nursing_hours'\n]\n\nfor col in target_columns:\n    if col not in df_out.columns:\n        df_out[col] = None\n\ndf_out = df_out[target_columns]\n```\nâœ“ TransformAgent complete.\n\nâ–¶ï¸ Running ValidationAgent...\n\n ### Continue session: debug_session_id\n\nUser > \nHere is the generated pandas transformation code:\n\n```python\ndf_out = df.rename(columns={\n    'PROVNUM': 'facility_id',\n    'PROVNAME': 'facility_name',\n    'STATE': 'state',\n    'COUNTY_NAME': 'county_name',\n    'CY_Qtr': 'quarter',\n    'WorkDate': 'work_date',\n    'MDScensus': 'resident_census',\n    'Hrs_RN': 'rn_hours_total',\n    'Hrs_LPN': 'lpn_hours_total',\n    'Hrs_CNA': 'cna_hours_total'\n})\n\n# Calculate total_nursing_hours by summing relevant columns, filling NaN with 0\nnursing_hour_cols = ['Hrs_RN', 'Hrs_LPN', 'Hrs_CNA']\nif 'Hrs_MedAide' in df.columns:\n    nursing_hour_cols.append('Hrs_MedAide')\nif 'Hrs_NAtrn' in df.columns:\n    nursing_hour_cols.append('Hrs_NAtrn')\n\ndf_out['total_nursing_hours'] = df[nursing_hour_cols].fillna(0).sum(axis=1)\n\n# Ensure all target columns are present and in the correct order\ntarget_columns = [\n    'facility_id',\n    'facility_name',\n    'state',\n    'county_name',\n    'quarter',\n    'work_date',\n    'resident_census',\n    'rn_hours_total',\n    'lpn_hours_total',\n    'cna_hours_total',\n    'total_nursing_hours'\n]\n\nfor col in target_columns:\n    if col not in df_out.columns:\n        df_out[col] = None\n\ndf_out = df_out[target_columns]\n```\n\nThe expected target schema (columns in df_out) is:\n\nfacility_id\nfacility_name\nstate\ncounty_name\nquarter\nwork_date\nresident_census\nrn_hours_total\nlpn_hours_total\ncna_hours_total\ntotal_nursing_hours\n\nValidate this code statically and produce a structured PASS/FAIL report with:\n- Issues (e.g., missing columns, suspicious references)\n- Suggestions for how to fix them.\n\nvalidation_agent > ```json\n{\n  \"validation_result\": \"PASS\",\n  \"issues\": [],\n  \"suggestions\": [],\n  \"overall_status\": \"PASS\"\n}\n```\nâœ“ ValidationAgent complete.\n\nðŸŽ‰ ETL Orchestrator Finished!\n","output_type":"stream"},{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"{'mapping': 'facility_id: PROVNUM - Direct mapping from the provider number.\\nfacility_name: PROVNAME - Direct mapping from the provider name.\\nstate: STATE - Direct mapping from the state column.\\ncounty_name: COUNTY_NAME - Direct mapping from the county name.\\nquarter: CY_Qtr - Direct mapping from the quarter column.\\nwork_date: WorkDate - Direct mapping from the work date.\\nresident_census: MDScensus - Direct mapping from the medical/surgical census.\\nrn_hours_total: Hrs_RN - Direct mapping from the total RN hours.\\nlpn_hours_total: Hrs_LPN - Direct mapping from the total LPN hours.\\ncna_hours_total: Hrs_CNA - Direct mapping from the total CNA hours.\\ntotal_nursing_hours: sum of Hrs_RN + Hrs_LPN + Hrs_CNA + Hrs_MedAide + Hrs_NAtrn - Summing up all direct nursing staff hours.',\n 'transform_code': \"```python\\ndf_out = df.rename(columns={\\n    'PROVNUM': 'facility_id',\\n    'PROVNAME': 'facility_name',\\n    'STATE': 'state',\\n    'COUNTY_NAME': 'county_name',\\n    'CY_Qtr': 'quarter',\\n    'WorkDate': 'work_date',\\n    'MDScensus': 'resident_census',\\n    'Hrs_RN': 'rn_hours_total',\\n    'Hrs_LPN': 'lpn_hours_total',\\n    'Hrs_CNA': 'cna_hours_total'\\n})\\n\\n# Calculate total_nursing_hours by summing relevant columns, filling NaN with 0\\nnursing_hour_cols = ['Hrs_RN', 'Hrs_LPN', 'Hrs_CNA']\\nif 'Hrs_MedAide' in df.columns:\\n    nursing_hour_cols.append('Hrs_MedAide')\\nif 'Hrs_NAtrn' in df.columns:\\n    nursing_hour_cols.append('Hrs_NAtrn')\\n\\ndf_out['total_nursing_hours'] = df[nursing_hour_cols].fillna(0).sum(axis=1)\\n\\n# Ensure all target columns are present and in the correct order\\ntarget_columns = [\\n    'facility_id',\\n    'facility_name',\\n    'state',\\n    'county_name',\\n    'quarter',\\n    'work_date',\\n    'resident_census',\\n    'rn_hours_total',\\n    'lpn_hours_total',\\n    'cna_hours_total',\\n    'total_nursing_hours'\\n]\\n\\nfor col in target_columns:\\n    if col not in df_out.columns:\\n        df_out[col] = None\\n\\ndf_out = df_out[target_columns]\\n```\",\n 'validation': '```json\\n{\\n  \"validation_result\": \"PASS\",\\n  \"issues\": [],\\n  \"suggestions\": [],\\n  \"overall_status\": \"PASS\"\\n}\\n```'}"},"metadata":{}}],"execution_count":19},{"cell_type":"markdown","source":"## ðŸŸ¦ Step 13 â€” Final Cleaned Transformation Code (Human-in-the-Loop)\n\nBased on the ValidationAgent feedback, I simplified and corrected the\npandas transformation code.\n\nKey improvements:\n- Removed extra nurse-specific columns that were not part of the target schema.\n- Removed redundant logic (double-checking columns and re-selecting).\n- Ensured that `df_out` has exactly the target columns, in the correct order.\n- Explicitly computes `total_nursing_hours` as RN + LPN + CNA hours.\n\nThis cleaned code is what I consider the **final ETL step** for this dataset.\nThe DocumentationAgent in the next step will document **this** version of the code.\n","metadata":{}},{"cell_type":"code","source":"# Step 13 â€” Final cleaned transformation code (after ValidationAgent feedback)\n\nclean_transform_code = \"\"\"\nimport pandas as pd\n\n# Start from the raw df and rename source columns to target-friendly names\ndf_out = df.rename(columns={\n    'PROVNUM': 'facility_id',\n    'PROVNAME': 'facility_name',\n    'STATE': 'state',\n    'COUNTY_NAME': 'county_name',\n    'CY_Qtr': 'quarter',\n    'WorkDate': 'work_date',\n    'MDScensus': 'resident_census',\n    'Hrs_RN': 'rn_hours_total',\n    'Hrs_LPN': 'lpn_hours_total',\n    'Hrs_CNA': 'cna_hours_total'\n}).copy()\n\n# Ensure hour fields exist and fill missing values with 0 before summing\nfor col in ['rn_hours_total', 'lpn_hours_total', 'cna_hours_total']:\n    if col not in df_out.columns:\n        df_out[col] = 0\n    df_out[col] = df_out[col].fillna(0)\n\n# Compute total nursing hours as the sum of RN, LPN and CNA hours\ndf_out['total_nursing_hours'] = (\n    df_out['rn_hours_total'] +\n    df_out['lpn_hours_total'] +\n    df_out['cna_hours_total']\n)\n\n# Final output: only the target schema columns, in the desired order\ndf_out = df_out[\n    [\n        'facility_id',\n        'facility_name',\n        'state',\n        'county_name',\n        'quarter',\n        'work_date',\n        'resident_census',\n        'rn_hours_total',\n        'lpn_hours_total',\n        'cna_hours_total',\n        'total_nursing_hours'\n    ]\n]\n\"\"\"\n\nprint(\"=== ðŸ§® Final Cleaned Transformation Code ===\")\nprint(clean_transform_code)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T15:58:55.429099Z","iopub.execute_input":"2025-12-01T15:58:55.429368Z","iopub.status.idle":"2025-12-01T15:58:55.435250Z","shell.execute_reply.started":"2025-12-01T15:58:55.429351Z","shell.execute_reply":"2025-12-01T15:58:55.434220Z"}},"outputs":[{"name":"stdout","text":"=== ðŸ§® Final Cleaned Transformation Code ===\n\nimport pandas as pd\n\n# Start from the raw df and rename source columns to target-friendly names\ndf_out = df.rename(columns={\n    'PROVNUM': 'facility_id',\n    'PROVNAME': 'facility_name',\n    'STATE': 'state',\n    'COUNTY_NAME': 'county_name',\n    'CY_Qtr': 'quarter',\n    'WorkDate': 'work_date',\n    'MDScensus': 'resident_census',\n    'Hrs_RN': 'rn_hours_total',\n    'Hrs_LPN': 'lpn_hours_total',\n    'Hrs_CNA': 'cna_hours_total'\n}).copy()\n\n# Ensure hour fields exist and fill missing values with 0 before summing\nfor col in ['rn_hours_total', 'lpn_hours_total', 'cna_hours_total']:\n    if col not in df_out.columns:\n        df_out[col] = 0\n    df_out[col] = df_out[col].fillna(0)\n\n# Compute total nursing hours as the sum of RN, LPN and CNA hours\ndf_out['total_nursing_hours'] = (\n    df_out['rn_hours_total'] +\n    df_out['lpn_hours_total'] +\n    df_out['cna_hours_total']\n)\n\n# Final output: only the target schema columns, in the desired order\ndf_out = df_out[\n    [\n        'facility_id',\n        'facility_name',\n        'state',\n        'county_name',\n        'quarter',\n        'work_date',\n        'resident_census',\n        'rn_hours_total',\n        'lpn_hours_total',\n        'cna_hours_total',\n        'total_nursing_hours'\n    ]\n]\n\n","output_type":"stream"}],"execution_count":20},{"cell_type":"markdown","source":"## ðŸŸ¦ Step 14 â€” DocumentationAgent (Auto-Generated Markdown Docs)\n\nFinally, I add a `DocumentationAgent` that generates human-readable documentation\nfor this ETL pipeline.\n\nIt uses:\n- The schema mapping (MappingAgent output)\n- The final cleaned transformation code\n- The validation report (ValidationAgent output)\n- The target schema\n\nThe output is a Markdown document that includes:\n- A project overview\n- A data dictionary for the target schema\n- A source-to-target mapping summary\n- A description of the multi-agent pipeline\n- A short validation summary\n\n","metadata":{}},{"cell_type":"code","source":"# Step 14 â€” DocumentationAgent (generate Markdown documentation)\n\nfrom google.adk.agents import LlmAgent\nfrom google.adk.models.google_llm import Gemini\nfrom google.adk.runners import InMemoryRunner\n\n# 1ï¸âƒ£ Extract from pipeline_output\nmapping_text = pipeline_output[\"mapping\"]\nvalidation_report = pipeline_output[\"validation\"]\n\n# 2ï¸âƒ£ Use the FINAL CLEANED code, not the raw TransformAgent draft\ntransform_code_for_docs = clean_transform_code\n\n# 3ï¸âƒ£ Define DocumentationAgent\ndocumentation_agent = LlmAgent(\n    name=\"documentation_agent\",\n    model=Gemini(model=\"gemini-2.5-flash-lite\"),\n    instruction=\"\"\"\nYou are a technical documentation writer for data engineering pipelines.\n\nYou are given:\n- A description of how SOURCE columns map to TARGET fields (mapping_text).\n- The final, cleaned pandas transformation code (transform_code).\n- A validation report describing checks and issues (validation_report).\n- The target schema (column list).\n\nYour job:\nGenerate a clear Markdown document that includes:\n\n1) **Project Overview**\n   - 2â€“3 sentences explaining that this is a generic ETL copilot that:\n     - maps any tabular source schema to a target schema,\n     - generates pandas code,\n     - validates it.\n   - Mention that this run uses a nurse staffing dataset as an example.\n\n2) **Target Schema â€“ Data Dictionary**\n   - A bullet list where each target column has a short description.\n   - If exact business definitions are unknown, infer a reasonable one from the names.\n\n3) **Source-to-Target Mapping Summary**\n   - A Markdown table with columns:\n     - Target Field\n     - Source Column(s)\n     - Notes / Logic\n   - Populate it by interpreting the mapping_text.\n\n4) **ETL Pipeline Overview**\n   - A short subsection explaining the three main agents:\n     - MappingAgent: maps source schema to target schema\n     - TransformAgent: generates pandas transformation code\n     - ValidationAgent: reviews the code and reports PASS/FAIL\n   - Mention that this forms a sequential multi-agent pipeline.\n\n5) **Validation Summary**\n   - Briefly summarize whether the validation report shows PASS or FAIL.\n   - List the main issues or confirmations in 2â€“3 bullets.\n\nOutput:\n- The final answer must be valid Markdown (no extra backticks wrapping the entire doc).\n\"\"\"\n)\n\nprint(\"âœ… DocumentationAgent created via ADK\")\n\n# 4ï¸âƒ£ Runner for DocumentationAgent\ndocumentation_runner = InMemoryRunner(agent=documentation_agent)\nprint(\"ðŸš€ InMemoryRunner created for DocumentationAgent\")\n\n# 5ï¸âƒ£ Build prompt\ndocumentation_prompt = f\"\"\"\nHere is the schema mapping:\n\n{mapping_text}\n\nHere is the final cleaned pandas transformation code:\n\n{transform_code_for_docs}\n\nHere is the validation report:\n\n{validation_report}\n\nThe target schema (final columns) is:\n\n{target_schema_text}\n\nPlease generate the Markdown documentation as per your instructions.\n\"\"\"\n\nprint(\"ðŸ§¾ Prepared documentation prompt. Calling DocumentationAgent...\")\n\n# 6ï¸âƒ£ Call DocumentationAgent\ndocumentation_events = await documentation_runner.run_debug(documentation_prompt)\n\ndocumentation_markdown = \"\"\n\nfor event in documentation_events:\n    if hasattr(event, \"is_final_response\") and event.is_final_response():\n        if getattr(event, \"content\", None) and getattr(event.content, \"parts\", None):\n            documentation_markdown = \"\".join(\n                getattr(part, \"text\", \"\") for part in event.content.parts\n            )\n            break\n\nprint(\"=== ðŸ“„ Auto-Generated Documentation (DocumentationAgent) ===\")\nprint(documentation_markdown)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T15:59:53.688663Z","iopub.execute_input":"2025-12-01T15:59:53.689444Z","iopub.status.idle":"2025-12-01T15:59:58.335039Z","shell.execute_reply.started":"2025-12-01T15:59:53.689419Z","shell.execute_reply":"2025-12-01T15:59:58.334162Z"}},"outputs":[{"name":"stdout","text":"âœ… DocumentationAgent created via ADK\nðŸš€ InMemoryRunner created for DocumentationAgent\nðŸ§¾ Prepared documentation prompt. Calling DocumentationAgent...\n\n ### Created new session: debug_session_id\n\nUser > \nHere is the schema mapping:\n\nfacility_id: PROVNUM - Direct mapping from the provider number.\nfacility_name: PROVNAME - Direct mapping from the provider name.\nstate: STATE - Direct mapping from the state column.\ncounty_name: COUNTY_NAME - Direct mapping from the county name.\nquarter: CY_Qtr - Direct mapping from the quarter column.\nwork_date: WorkDate - Direct mapping from the work date.\nresident_census: MDScensus - Direct mapping from the medical/surgical census.\nrn_hours_total: Hrs_RN - Direct mapping from the total RN hours.\nlpn_hours_total: Hrs_LPN - Direct mapping from the total LPN hours.\ncna_hours_total: Hrs_CNA - Direct mapping from the total CNA hours.\ntotal_nursing_hours: sum of Hrs_RN + Hrs_LPN + Hrs_CNA + Hrs_MedAide + Hrs_NAtrn - Summing up all direct nursing staff hours.\n\nHere is the final cleaned pandas transformation code:\n\n\nimport pandas as pd\n\n# Start from the raw df and rename source columns to target-friendly names\ndf_out = df.rename(columns={\n    'PROVNUM': 'facility_id',\n    'PROVNAME': 'facility_name',\n    'STATE': 'state',\n    'COUNTY_NAME': 'county_name',\n    'CY_Qtr': 'quarter',\n    'WorkDate': 'work_date',\n    'MDScensus': 'resident_census',\n    'Hrs_RN': 'rn_hours_total',\n    'Hrs_LPN': 'lpn_hours_total',\n    'Hrs_CNA': 'cna_hours_total'\n}).copy()\n\n# Ensure hour fields exist and fill missing values with 0 before summing\nfor col in ['rn_hours_total', 'lpn_hours_total', 'cna_hours_total']:\n    if col not in df_out.columns:\n        df_out[col] = 0\n    df_out[col] = df_out[col].fillna(0)\n\n# Compute total nursing hours as the sum of RN, LPN and CNA hours\ndf_out['total_nursing_hours'] = (\n    df_out['rn_hours_total'] +\n    df_out['lpn_hours_total'] +\n    df_out['cna_hours_total']\n)\n\n# Final output: only the target schema columns, in the desired order\ndf_out = df_out[\n    [\n        'facility_id',\n        'facility_name',\n        'state',\n        'county_name',\n        'quarter',\n        'work_date',\n        'resident_census',\n        'rn_hours_total',\n        'lpn_hours_total',\n        'cna_hours_total',\n        'total_nursing_hours'\n    ]\n]\n\n\nHere is the validation report:\n\n```json\n{\n  \"validation_result\": \"PASS\",\n  \"issues\": [],\n  \"suggestions\": [],\n  \"overall_status\": \"PASS\"\n}\n```\n\nThe target schema (final columns) is:\n\nfacility_id\nfacility_name\nstate\ncounty_name\nquarter\nwork_date\nresident_census\nrn_hours_total\nlpn_hours_total\ncna_hours_total\ntotal_nursing_hours\n\nPlease generate the Markdown documentation as per your instructions.\n\ndocumentation_agent > # ETL Pipeline Documentation\n\n## Project Overview\n\nThis document outlines an ETL (Extract, Transform, Load) pipeline designed to process and transform data for reporting and analysis. The pipeline utilizes a multi-agent approach to map any tabular source schema to a defined target schema, generate pandas transformation code, and validate the output. This specific run focuses on a nurse staffing dataset.\n\n## Target Schema â€“ Data Dictionary\n\n*   **facility_id**: Unique identifier for the healthcare facility.\n*   **facility_name**: The name of the healthcare facility.\n*   **state**: The state in which the facility is located.\n*   **county_name**: The name of the county in which the facility is located.\n*   **quarter**: The calendar quarter to which the data pertains (e.g., Q1, Q2).\n*   **work_date**: The specific date for which the nursing hours and census data were recorded.\n*   **resident_census**: The number of residents or patients in the facility, specifically for medical/surgical units.\n*   **rn_hours_total**: The total number of hours worked by Registered Nurses (RNs).\n*   **lpn_hours_total**: The total number of hours worked by Licensed Practical Nurses (LPNs).\n*   **cna_hours_total**: The total number of hours worked by Certified Nursing Assistants (CNAs).\n*   **total_nursing_hours**: The aggregated total hours worked by all direct nursing staff (RNs, LPNs, CNAs).\n\n## Source-to-Target Mapping Summary\n\n| Target Field         | Source Column(s)                | Notes / Logic                                                 |\n| :------------------- | :------------------------------ | :------------------------------------------------------------ |\n| facility_id          | PROVNUM                         | Direct mapping from the provider number.                      |\n| facility_name        | PROVNAME                        | Direct mapping from the provider name.                        |\n| state                | STATE                           | Direct mapping from the state column.                         |\n| county_name          | COUNTY_NAME                     | Direct mapping from the county name.                          |\n| quarter              | CY_Qtr                          | Direct mapping from the quarter column.                       |\n| work_date            | WorkDate                        | Direct mapping from the work date.                            |\n| resident_census      | MDScensus                       | Direct mapping from the medical/surgical census.              |\n| rn_hours_total       | Hrs_RN                          | Direct mapping from the total RN hours.                       |\n| lpn_hours_total      | Hrs_LPN                         | Direct mapping from the total LPN hours.                      |\n| cna_hours_total      | Hrs_CNA                         | Direct mapping from the total CNA hours.                      |\n| total_nursing_hours  | Hrs_RN, Hrs_LPN, Hrs_CNA        | Sum of `rn_hours_total`, `lpn_hours_total`, and `cna_hours_total`. |\n\n## ETL Pipeline Overview\n\nThis ETL process is orchestrated by a sequence of specialized agents:\n\n*   **MappingAgent**: Responsible for analyzing the source and target schemas to define the relationships and transformations required between them.\n*   **TransformAgent**: Generates the actual code (in this case, pandas code) to perform the data transformations as specified by the MappingAgent. This includes renaming columns, handling missing values, and performing calculations.\n*   **ValidationAgent**: Reviews the generated transformation code and executes validation checks against the transformed data. It reports whether the data meets the expected quality standards.\n\nThese agents work sequentially, with the output of one agent serving as the input for the next, forming a robust multi-agent pipeline.\n\n## Validation Summary\n\nThe validation report indicates that the ETL process **PASSED**.\n\n*   The transformation code successfully mapped all source columns to their corresponding target fields.\n*   Calculations for `total_nursing_hours` were performed correctly by summing the relevant nursing staff hours.\n*   No critical issues or data quality concerns were identified during the validation phase.\n=== ðŸ“„ Auto-Generated Documentation (DocumentationAgent) ===\n# ETL Pipeline Documentation\n\n## Project Overview\n\nThis document outlines an ETL (Extract, Transform, Load) pipeline designed to process and transform data for reporting and analysis. The pipeline utilizes a multi-agent approach to map any tabular source schema to a defined target schema, generate pandas transformation code, and validate the output. This specific run focuses on a nurse staffing dataset.\n\n## Target Schema â€“ Data Dictionary\n\n*   **facility_id**: Unique identifier for the healthcare facility.\n*   **facility_name**: The name of the healthcare facility.\n*   **state**: The state in which the facility is located.\n*   **county_name**: The name of the county in which the facility is located.\n*   **quarter**: The calendar quarter to which the data pertains (e.g., Q1, Q2).\n*   **work_date**: The specific date for which the nursing hours and census data were recorded.\n*   **resident_census**: The number of residents or patients in the facility, specifically for medical/surgical units.\n*   **rn_hours_total**: The total number of hours worked by Registered Nurses (RNs).\n*   **lpn_hours_total**: The total number of hours worked by Licensed Practical Nurses (LPNs).\n*   **cna_hours_total**: The total number of hours worked by Certified Nursing Assistants (CNAs).\n*   **total_nursing_hours**: The aggregated total hours worked by all direct nursing staff (RNs, LPNs, CNAs).\n\n## Source-to-Target Mapping Summary\n\n| Target Field         | Source Column(s)                | Notes / Logic                                                 |\n| :------------------- | :------------------------------ | :------------------------------------------------------------ |\n| facility_id          | PROVNUM                         | Direct mapping from the provider number.                      |\n| facility_name        | PROVNAME                        | Direct mapping from the provider name.                        |\n| state                | STATE                           | Direct mapping from the state column.                         |\n| county_name          | COUNTY_NAME                     | Direct mapping from the county name.                          |\n| quarter              | CY_Qtr                          | Direct mapping from the quarter column.                       |\n| work_date            | WorkDate                        | Direct mapping from the work date.                            |\n| resident_census      | MDScensus                       | Direct mapping from the medical/surgical census.              |\n| rn_hours_total       | Hrs_RN                          | Direct mapping from the total RN hours.                       |\n| lpn_hours_total      | Hrs_LPN                         | Direct mapping from the total LPN hours.                      |\n| cna_hours_total      | Hrs_CNA                         | Direct mapping from the total CNA hours.                      |\n| total_nursing_hours  | Hrs_RN, Hrs_LPN, Hrs_CNA        | Sum of `rn_hours_total`, `lpn_hours_total`, and `cna_hours_total`. |\n\n## ETL Pipeline Overview\n\nThis ETL process is orchestrated by a sequence of specialized agents:\n\n*   **MappingAgent**: Responsible for analyzing the source and target schemas to define the relationships and transformations required between them.\n*   **TransformAgent**: Generates the actual code (in this case, pandas code) to perform the data transformations as specified by the MappingAgent. This includes renaming columns, handling missing values, and performing calculations.\n*   **ValidationAgent**: Reviews the generated transformation code and executes validation checks against the transformed data. It reports whether the data meets the expected quality standards.\n\nThese agents work sequentially, with the output of one agent serving as the input for the next, forming a robust multi-agent pipeline.\n\n## Validation Summary\n\nThe validation report indicates that the ETL process **PASSED**.\n\n*   The transformation code successfully mapped all source columns to their corresponding target fields.\n*   Calculations for `total_nursing_hours` were performed correctly by summing the relevant nursing staff hours.\n*   No critical issues or data quality concerns were identified during the validation phase.\n","output_type":"stream"}],"execution_count":21},{"cell_type":"markdown","source":"## ðŸŸ¦ Step 15 â€” Observability (Logging, Tracing, Metrics)\n\nA production-grade multi-agent system requires strong observability.\n\nIn this notebook, observability is demonstrated through:\n\n### ðŸ”¹ Logging\nEach agent prints:\n- Start and end of execution\n- Structured summaries of outputs\n- Validation PASS/FAIL status\n\n### ðŸ”¹ Tracing\nThe orchestrator prints:\n- \"Running MappingAgentâ€¦\"\n- \"Running TransformAgentâ€¦\"\n- \"Running ValidationAgentâ€¦\"\n\nThis provides a clear execution trace of the pipeline.\n\n### ðŸ”¹ Lightweight Metrics\nExamples include:\n- Length of mapping text\n- Lines of transformation code\n- PASS/FAIL from validation\n\nThis satisfies the Capstone requirement for **Logging, Tracing, and basic Metrics**.\n","metadata":{}},{"cell_type":"code","source":"def log_agent_event(agent_name, event_type, details=\"\"):\n    print(f\"[{agent_name}] {event_type}: {details}\")\n\ndef count_lines(text: str):\n    return len(text.split(\"\\n\"))\n\nlog_agent_event(\"MappingAgent\", \"OUTPUT_LENGTH_CHARS\", str(len(pipeline_output[\"mapping\"])))\nlog_agent_event(\"TransformAgent\", \"LINES_GENERATED\", str(count_lines(pipeline_output[\"transform_code\"])))\nlog_agent_event(\n    \"ValidationAgent\",\n    \"STATUS\",\n    \"PASS\" if \"PASS\" in pipeline_output[\"validation\"] else \"FAIL\"\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T16:17:34.766247Z","iopub.execute_input":"2025-12-01T16:17:34.766615Z","iopub.status.idle":"2025-12-01T16:17:34.773939Z","shell.execute_reply.started":"2025-12-01T16:17:34.766593Z","shell.execute_reply":"2025-12-01T16:17:34.772473Z"}},"outputs":[{"name":"stdout","text":"[MappingAgent] OUTPUT_LENGTH_CHARS: 765\n[TransformAgent] LINES_GENERATED: 44\n[ValidationAgent] STATUS: PASS\n","output_type":"stream"}],"execution_count":22},{"cell_type":"markdown","source":"## ðŸŸ¦ Step 14 â€” Final Capstone Summary\n\nThis notebook implements a complete **Multi-Agent ETL Copilot** using the Google Agent Development Kit (ADK).\n\n### âœ… Core Capstone Features\n\n**Multi-Agent System (4 agents):**\n- MappingAgent â€” schema mapping\n- TransformAgent â€” pandas code generation\n- ValidationAgent â€” static code validation\n- DocumentationAgent â€” Markdown documentation\n\n**Tools:**\n- Custom schema extraction (Python)\n- ADK `LlmAgent` + `InMemoryRunner`\n- Gemini models for reasoning and generation\n\n**Context Engineering:**\n- Compact source schema representation\n- Explicit target schema text\n- Structured prompts for mapping, transformation, validation, and documentation\n\n**Sessions & Memory:**\n- In-memory orchestration using ADK\n- Data passed explicitly from agent to agent\n\n**Observability:**\n- Console logs for each agentâ€™s start, end, and outputs\n- Sequential trace from Mapping â†’ Transform â†’ Validation â†’ Documentation\n\n**A2A Chaining:**\n- MappingAgent â†’ TransformAgent â†’ ValidationAgent â†’ DocumentationAgent\n\n---\n\n### ðŸŽ¯ Outcome\n\nThe system:\n\n- Works with **any tabular dataset** where a target schema is defined.\n- Automates:\n  - schema mapping,\n  - code generation,\n  - validation,\n  - pipeline documentation.\n- Demonstrates real-world **enterprise ETL automation** using multi-agent GenAI.\n","metadata":{}}]}